{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.1: Text Cleaning\n",
    "def clean_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Step 2.2: Tokenization\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Step 2.3: Stopword Removal\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\"Hi anshika, I hate you. I cannot tolerate you.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = [clean_text(text) for text in text_data]\n",
    "tokenized_text = [tokenize_text(text) for text in cleaned_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text = [remove_stopwords(tokens) for tokens in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Text:\", text_data)\n",
    "print(\"Cleaned Text:\", cleaned_text)\n",
    "print(\"Tokenized Text:\", tokenized_text)\n",
    "print(\"Text after Stopword Removal:\", filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "try:\n",
    "    db_connection = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"root\",\n",
    "        database=\"AI_hate\"\n",
    "    )\n",
    "    print(\"Connected to the database successfully!\")\n",
    "except mysql.connector.Error as e:\n",
    "    print(\"Error connecting to the database:\", e)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = db_connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT * FROM ChatScreenshots\")\n",
    "rows = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_rows = random.sample(rows, 25)\n",
    "preprocessed_texts = []\n",
    "hate_speech_labels = []\n",
    "# Tokenize the text data for each tuple\n",
    "for row in random_rows:\n",
    "    text_data = row[1]  \n",
    "    tokenized_text = word_tokenize(text_data)\n",
    "    preprocessed_text = \" \".join(tokenized_text)  # Convert tokenized text to string\n",
    "    preprocessed_texts.append(preprocessed_text)\n",
    "    hate_speech_labels.append(row[2])  # Assuming the label is in the second column of the table\n",
    "    print(\"Original Text:\", text_data)\n",
    "    print(\"Tokenized Text:\", tokenized_text)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(preprocessed_texts) != len(hate_speech_labels):\n",
    "    print(\"Inconsistent number of preprocessed texts and labels.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_texts, hate_speech_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from textblob import TextBlob\n",
    "import pytesseract\n",
    "import re\n",
    "import mysql.connector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Connect to the MySQL database\n",
    "try:\n",
    "    db_connection = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"root\",\n",
    "        database=\"AI_hate\"\n",
    "    )\n",
    "    print(\"Connected to the database successfully!\")\n",
    "except mysql.connector.Error as e:\n",
    "    print(\"Error connecting to the database:\", e)\n",
    "    exit()\n",
    "\n",
    "# Step 2.1: Text Cleaning\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Step 2.2: Tokenization\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Step 2.3: Stopword Removal\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    # Initialize TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the training data\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # Initialize logistic regression classifier\n",
    "    classifier = LogisticRegression()\n",
    "\n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    return classifier, tfidf_vectorizer\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    sentiment_score = analysis.sentiment.polarity\n",
    "    if any(word.isupper() for word in text.split()):\n",
    "        sentiment_score -= 0.1  # Decrease sentiment score by 0.1 if capital words are present\n",
    "    \n",
    "    if sentiment_score > 0:\n",
    "        return \"Positive\"\n",
    "    elif sentiment_score < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "def detect_hate_speech(text, classifier, hate_speech_phrases):\n",
    "    # Clean and preprocess the input text\n",
    "    cleaned_text = clean_text(text)\n",
    "    tokenized_text = tokenize_text(cleaned_text)\n",
    "    filtered_text = remove_stopwords(tokenized_text)\n",
    "    \n",
    "    # Convert the preprocessed text into TF-IDF vector\n",
    "    text_tfidf = tfidf_vectorizer.transform([' '.join(filtered_text)])\n",
    "    \n",
    "    # Predict using the trained classifier\n",
    "    prediction = classifier.predict(text_tfidf)\n",
    "    \n",
    "    # If the prediction is hate speech (1), return True\n",
    "    if prediction[0] == 1:\n",
    "        return True\n",
    "    \n",
    "    # Otherwise, continue with database lookup\n",
    "    for phrase in hate_speech_phrases:\n",
    "        if re.search(phrase, text, flags=re.IGNORECASE):\n",
    "            return True\n",
    "    \n",
    "    # If no hate speech detected, return False\n",
    "    return False\n",
    "\n",
    "def ocr_and_detect(image_path, classifier, hate_speech_phrases):\n",
    "    # Perform OCR using Tesseract\n",
    "    detected_text = pytesseract.image_to_string(Image.open(image_path))\n",
    "\n",
    "    # Detect hate speech\n",
    "    if detect_hate_speech(detected_text, classifier, hate_speech_phrases):\n",
    "        print(\"Hate speech detected in the image!\")\n",
    "        sentiment = analyze_sentiment(detected_text)\n",
    "        print(\"Sentiment of the detected text:\", sentiment)\n",
    "    else:\n",
    "        print(\"No hate speech detected in the image.\")\n",
    "\n",
    "    return detected_text;\n",
    "\n",
    "# Example SQL query to fetch text data from a table\n",
    "cursor = db_connection.cursor()\n",
    "cursor.execute(\"SELECT * FROM ChatScreenshots\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Train model\n",
    "texts = [row[1] for row in rows]\n",
    "labels = [row[2] for row in rows]\n",
    "classifier, tfidf_vectorizer = train_model(texts, labels)\n",
    "\n",
    "# Get hate speech phrases from the database\n",
    "cursor.execute(\"SELECT content FROM ChatScreenshots WHERE label = 'hate speech'\")\n",
    "hate_speech_phrases = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "# Ask for image path\n",
    "image_path = input(\"Enter the path to the image file: \").strip('\"')\n",
    "\n",
    "# Perform OCR and hate speech detection\n",
    "st=ocr_and_detect(image_path, classifier, hate_speech_phrases)\n",
    "print(st)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
